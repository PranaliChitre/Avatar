# Avatar

Link : https://drive.google.com/drive/folders/1WdoYTyfPp0a-gsY3_bkk4RaHg8evRwdk?usp=sharing

Tools Used:
1.Stable Diffusion (Image Generation):
Purpose: To generate high-quality, AI-driven images based on text prompts.
Details: Stable Diffusion is an open-source deep learning model designed to produce detailed and realistic images. It was utilized to create the initial visual content for the project.

2.LeiaPix Converter (Image Animation):
Purpose: To animate static images generated by Stable Diffusion.
Details: LeiaPix Converter transforms static images into dynamic, animated visuals, adding movement and life to the generated content.

3.Eleven Labs (Text-to-Speech):
Purpose: To convert text into natural, human-like speech.
Details: Eleven Labs offers advanced AI-driven text-to-speech technology, producing high-quality voiceovers that sound realistic and engaging.

4.Wav2Lip (Lip Synchronization):
Purpose: To synchronize lip movements in video with the generated speech.
Details: Wav2Lip is an open-source tool that precisely aligns lip movements with audio input, ensuring that the speech and visual elements in the video are perfectly synchronized.

Process Followed:
1.Wav2Lip Setup:
Installation: Began by installing the necessary dependencies and cloning the Wav2Lip repository.
Model Downloads: Downloaded the pretrained Wav2Lip model (wav2lip_gan.pth) and the face detection model (s3fd.pth).
Additional Libraries: Installed essential libraries, including youtube-dl, ffmpeg-python, and librosa, to support the processing and synchronization tasks.

2.Preparation of Input Files:
Uploaded the input video (input_video.mp4) and audio (input_audio.wav) files, which served as the basis for the lip synchronization process.

3.Lip-Synced Video Creation:
Execution: Utilized the Wav2Lip model to generate a video where the lip movements of the subject were precisely aligned with the provided audio track. This was achieved using the command python inference.py with specified parameters.

4.Result Review:
Playback: The resulting video (result_voice.mp4) was played back at 50% scaling to assess the accuracy and quality of the lip synchronization.
